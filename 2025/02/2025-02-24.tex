\documentclass[report]{../../custom}
\begin{document}
\maketitle

\noindent \textbf{摘要：}
本周完成了SHA256算法的CUDA并行实现与性能分析实验，实现了二种不同的GPU并行化方案。在RTX 4090平台上，最优配置(Grid=128, Block=256)达到了478,324 MB/s的吞吐量，相比CPU实现获得2076倍加速比。通过详细的性能分析，发现消息大小与线程配置对计算性能有显著影响，并确定了最佳运行参数。同时完成了论文前置知识章节的撰写，主要包括SPHINCS\textsuperscript{+}签名方案的结构组成和GPU计算模型的基础架构介绍。

\vskip 0.5cm

\noindent \textbf{下周计划：} 1) 将动态线程配置策略从HASH函数扩展到整个SPHINCS\textsuperscript{+}签名过程，以进一步提升系统性能；2) 探索GPU多流处理技术，提高GPU计算资源的利用率。

\section{SHA256 实验}

本实验实现了使用CUDA进行SHA256哈希计算的并行加速。实验采用了三种不同的实现方式：批处理、数据并行处理和多流处理。实验在配备RTX 4090 GPU（128个SM，16,384个CUDA核心）的环境下进行。

\subsection{最大吞吐量总结}

实验测试了不同配置下SHA256哈希计算的性能表现，包括CPU单核、GPU单核、GPU并行以及GPU多流处理等多种实现方式。其中GPU并行和多流实现尝试了不同的线程配置组合，以下是各种实现方式的最佳性能数据：

\begin{table}[htbp]
\centering
\caption{不同实现方式的最大吞吐量对比}
\begin{tabular}{|l|r|r|r|}
\hline
实现方式 & 最大吞吐量 (MB/s) & 消息大小 (B) & 相对CPU加速比 \\
\hline
CPU单核 \cite{Wang2025} & 230.40 & 131,072 & 1× \\
GPU单核 \cite{Wang2025} & 25.39 & 16,384 & 0.11× \\
\hline
GPU并行(128*256) & 478,324.05 & 1,024 & 2,076× \\
\hline
GPU多流(128*1024*8) & 22,923.71 & 16,384 & 99× \\
\hline
\end{tabular}
\end{table}

从最大吞吐量的对比可以看出，GPU单核性能反而低于CPU单核，这是由于GPU单线程执行效率较低，且存在额外的数据传输开销。在所有配置中，Grid大小为128、Block大小为256的GPU并行实现获得了最佳性能，在处理1024B消息时达到了478,324.05 MB/s的吞吐量，是\textcolor{blue}{CPU性能的2076倍}。

\subsection{性能分析}

\subsubsection{GPU并行实现性能对比}

不同线程配置下的GPU并行实现测试结果如下：

\begin{table}[htbp]
\centering
\caption{不同线程配置的GPU并行实现性能对比}
\begin{tabular}{|l|r|r|r|}
\hline
线程配置 & 最大吞吐量 (MB/s) & 消息大小 (B) & 相对CPU加速比 \\
\hline
GPU并行(82*512) & 320,969.49 & 2,048 & 1,393× \\
GPU并行(128*256) & 478,324.05 & 1,024 & 2,076× \\
GPU并行(128*1024) & 316,416.92 & 512 & 1,374× \\
GPU并行(256*1024) & 349,549.00 & 512 & 1,517× \\
\hline
\end{tabular}
\end{table}
从性能测试结果分析可以得出以下结论：128*256的配置获得了最佳性能，在处理1024B消息时达到了478,324.05 MB/s的吞吐量。增加Block大小（从256到1024）反而导致性能下降，表明更大的线程块可能引起更多的资源竞争。网格大小为128时性能较优，这与RTX 4090的128个SM完美匹配。所有配置在较小消息大小（512B-2048B）时达到峰值性能，这表明此时计算资源和内存访问达到最佳平衡。相比CPU实现，GPU并行实现获得了1,374×到2,076×不等的性能提升。

这些结果印证了之前的配置分析，即\textcolor{blue}{Grid大小需要匹配SM数量，而Block大小需要在资源利用和竞争之间取得平衡}。较大的Block size虽然理论上可以提供更多的并行性，但实际上可能因为资源争用而降低整体性能。

\subsubsection{吞吐量变化分析}

根据GPU并行(128*256)配置的实验数据，我们可以清晰地观察到吞吐量随消息大小变化的三个阶段：

\begin{table}[htbp]
\centering
\caption{消息大小对SHA256哈希计算吞吐量的影响}
\begin{tabular}{|l|r|r|r|}
\hline
阶段 & 消息大小范围 & 吞吐量范围 (MB/s) & 典型性能 \\
\hline
初始增长期 & 4B-512B & 1,044-352,862 & 小消息大小下快速提升 \\
峰值性能期 & 1024B-4096B & 457,400-478,324 & 达到最佳平衡状态 \\
性能下降期 & >4096B & 425,784-461,580 & 趋于稳定并小幅波动 \\
\hline
\end{tabular}
\end{table}

\textbf{初始增长期（4B-512B）}：在这个阶段，吞吐量从1,044MB/s快速增长至352,862MB/s。这种快速增长主要是因为内核启动和内存传输等固定开销被更多数据分摊，同时硬件利用率随数据量增加而提高。
\textbf{峰值性能期（1024B-4096B）}：在此阶段达到性能最优，特别是在1024B时达到最高吞吐量478,324MB/s。此时GPU内存带宽（1,008 GB/s）被充分利用，计算资源与内存访问达到最佳平衡。
\textbf{性能下降期（>4096B）}：当消息大小继续增加时，吞吐量在425,784-461,580MB/s之间波动。这主要是因为内存带宽成为主要瓶颈，同时缓存失效率增加，大消息处理的内存延迟影响也随之加大。
总的来说，相同的并发配置下，单个运算规模的大小（此处为消息长度），对整体性能有显著影响。因此\textcolor{blue}{动态的调整线程配置和消息分配具有研究价值。}

\section{前置知识写作}

完成了论文前置知识写作，主要包含两部分：SPHINCS\textsuperscript{+}概述和GPU计算模型。

SPHINCS\textsuperscript{+}是无状态的基于哈希的后量子签名方案，由三个核心组件构成：
WOTS+：一次性签名方案，用于认证路径,
FORS：少次签名方案，使用$k$个$t$元素的伪随机子集,
Hypertree：$h$高度的$d$层结构，每层包含$h/d$高度的默克尔树

GPU计算模型方面：
硬件由多个流式多处理器(SM)组成，每个SM包含多个CUDA核心,
采用SIMT执行模式，线程组织为warp和block结构,
包含共享内存、寄存器和缓存等多级存储系统,
CUDA框架提供合并访存等优化策略。

%% if you want to use biblatex, the uncomment the following lines
\bibliographystyle{alpha}
\bibliography{../../paper}

\end{document}